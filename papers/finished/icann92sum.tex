%% Extended abstract for ICANN'92 (Brighton)
%%
\documentstyle[icann92]{article}

\begin{document}
\bibliographystyle{theapa}

\title{The Speed and Slips of Mental Arithmetic}
\date{}
\author{Richard Dallaway}
\maketitle

\noindent{\large\bf Category\hspace{2mm}} Cognitive models.

\sec{Novelty} The literature on arithmetic memory offers a number of
theories for how we succeed and fail in recalling basic facts (such as
$\x34=12$). \citeA{mcclmode} have noted that these theories lack detailed
implementations, and as such are difficult to appraise.  This paper
presents a connectionist model of memory for multiplication facts which
replicates both error patterns and reaction times (RTs) for human adult
subjects. It does this using far fewer assumptions than many theories
presume.

This is the first explicit model to present a detailed statistical
comparison to human multiplication performance.  By mirroring published
human experiments we have been forced to fully specify the details of the
model (e.g., the mechanism by which a response is selected). Previous
accounts have proliferated assumptions or been vague. It is hoped that the
account presented here is detailed and minimal enough to begin the
assessment of existing theories.\medskip

\noindent{\large\bf Last
paper\hspace{2mm}} This work has not been published.

\sec{Summary}

\paragraph{Introduction} Adults
exhibit well documented patterns of behaviour when recalling
multiplication facts. RTs generally increase across the multiplication
tables: problems in the nine times table tend to take longer to answer than
problems in the two times table. However, this ``problem size effect'' has
plenty of exceptions (see figure~\ref{rtplot}). Tie problems (\x22, \x33
etc.) are recalled relatively quickly. In addition to this
\citeA{camp85} found that adults make errors at the rate of 7.65 per cent,
and of those errors 92.6 per cent fall into the following categories:
\begin{itemize}
\item Operand errors, for which the erroneous product for the problem \x a
b is a member of the $a$ or $b$ multiplication table (e.g., $\x65=45$).
\item The operand distance effect: errors are usually operand errors, and
also close in magnitude to the correct product. That is, for the problem \x
a b, the error will often be correct for the problem $a\pm2\times b$ or
$a\times b\pm2$ (e.g., $\x65=40$).
\item A frequent product is one of the five products 12, 16, 18, 24 or 36.
\item Table errors, where the erroneous product is the correct answer to
some problem, but does not share any digits with the presented
problem (e.g., \x65=16).
\item Operation errors, where the error to \x a b is correct for $a+b$.
\end{itemize}

\begin{figure}[bht]\centerline{\fbox{\vbox{\vspace{5mm}%
\centerfig{../postscript/xrt.ps}[58]
\caption{Plot of mean correct RT per multiplication table collapsed over
operand order for mean RT of: 60 adults \protect\cite[app.~A]{camp85};
20 networks trained on skewed frequencies; and, the same 20 networks after
continued training on uniform frequencies. The RT for both networks has
been scaled by the same amount.}
\label{rtplot}}}} \end{figure}

Although we all learn
the multiplication tables, we sometimes make mistakes.
The problem is to produce a model which has correctly learnt the
tables, yet can make slips when recalling answers. Given the
observations on the types of erroneous responses, and the RT for correct
responses, what assumptions must such a model make?
The model presented here suggests that the initial skew in the frequency
and order of presentation of multiplication fact is
one of the important factors.

\paragraph{Architecture} The network is trained on all the problems \x22
through \x99 in a random order using backpropagation. The two digits that
comprise a problem are coarse encoded on the two sets of eight input units
(representing the digits 2--9).  For tie problems, an additional tie bit is
set to 1.  Activation flows through a hidden layer of ten units, to the
output layer.  There is one output unit per product, plus a ``don't
know'' unit.

During training, the presentation frequency of each pattern is
linearly skewed in favour of the small product problems. Although small
problems do occur more frequently in textbooks, there is no reason to
believe this skew continues into adulthood \cite[p. 328]{mcclmode}.  Hence,
after training on the skewed data, the network is trained on equal
frequencies.

The ``cascade'' equations \cite[p.~153]{pdp3} are used to simulate the
spread of activation in the network.  Each unit's activity is allowed to
build up over time:
\def\net{\mbox{net}}
$$\net_i(t) = k \sum\limits_j w_{ij} a_j(t) + (1-k)\ \net_i(t-1),$$
where $k$ is the
cascade rate (0.05) which determines the rate with which activation builds
up, $w$ is the weight matrix, and $a_j(t)$ is the activation of unit $j$ at
time $t$. When $k = 1.0$ the network computes a standard feed-forward pass.
The $\net_i$ is passed through a logistic squashing function to produce the
activation value, $a_j$, and the response values are taken to be the
normalized activation values.

\paragraph{Simulations} At the start of cascade processing the initial
state of the network is the state that results from processing an all-zeros
input pattern.  The network is trained to activate the ``don't know'' unit
for all-zeros input. For each problem the network randomly selects a
threshold between 0.4 and 0.9. Processing continues until an product unit
exceeds the threshold. The RT (number of cascade steps) is recorded for a
correct response, and erroneous responses are classified into the five
categories itemized above. The network is presented with each of the 64
problems 50 times, and the mean correct RT is recorded.  This is repeated
with 20 different networks (different initial weights).

With a large threshold the correct product is recalled, but
occasionally the threshold will be low enough to accept an incorrect
product (see figure~\ref{actplot}). Presumably the mild time pressure of
the experimental situation results in subjects lowering their thresholds.

\begin{figure}[bht]\centerline{\fbox{\vbox{\vspace{5mm}%
\centerfig{../postscript/p3x8.ps}[55]%\vspace{4.2cm}
\caption{Response of the output units over 40 time steps for the problem
\x38.  Output units representing products over 32 are not shown on this
graph.}\label{actplot}}}}\end{figure}

\paragraph{Results} The mean RTs plotted in figure~\ref{rtplot} show some
of the basic features of the problem size effect.  For the skewed networks
the RT correlates $r=0.32$ ($p=0.0046$) with adult RT \cite{camp85}. This
falls to $r=0.19$ ($p=0.065$) after substantial training on the equalized
patterns.  Note that the RTs have reduced and flattened out for the
equalized network, which is just what is expected after continued practice
\cite[p.~349]{camp85}. The obvious feature of the RT plot is the drop in RT
for the nine times table.  Children in grades 3 to~5 respond faster to \X9
than \X8 problems \cite{camp85}, but this levels out for adults.

\input{ ../inputs/errorpc.tex } Table~\ref{errorpc} shows the error
percentages of the networks compared to those of adults.  Both sets of
networks have error distributions that are similar to that of adults, and
there is little difference between the skewed and equalized
networks.

A further point of interest is the correlation between problem error rate
and correct RT. \citeA[p.~110]{camprole} reports a correlation of 0.93 for
adults.  For the skewed and equalized networks $r=0.76$
and $r=0.77$ respectively.  It is not obvious that any model
would necessarily predict that slower problems produce more errors.

In all it is surprising that such a simple model can replicate a subset of
the results from human experiments. Alternative simulations demonstrated
the importance of the initial skew in presentation frequency, and further
assumptions were not required.  In particular there was no need to
explicitly train erroneous products as some theories suggest
\cite{siegmult,camp85}.
%That computational mechanisms can flesh out minimum
%requirements for a phenomenon bears testimony to the importance of
%implementation.

\sec{Previous work} The most successful models of mental arithmetic are the
``network'' models.  These models view the phenomena as spread of
activation, and so are comparable to connectionist models. What the
``network'' models lack is detailed implementations to test their
assumptions. The ``distributions of associations'' model \cite{siegmult} is
the most well defined and comprehensive of the ``network'' models. A less
developed account is given by \citeA{ashcchil}.

There is another connectionist model of mental multiplication
\cite{andestud,viscrepr} based around the brain-state-in-a-box
(BSB) algorithm.   The full details of the BSB model have not yet been
published, and only small scale simulations have been performed.  In
essence, the model is an auto-associator that settles into attractor states
representing the answer to the presented problem.  However, this means that
the model, as presented, simply lacks the ability to correctly answer some
problems, or fails to respond at all.  This runs against the notion that
slips are one-off run-time errors, rather than permanent disabilities.
\citeA{mcclmode} comment that the \citeA{viscrepr} ``proposal has several
limitations and cannot be considered a well-articulated model'', but add
that ``the [neural net] approach probably merits further exploration''
[p.~395].

\bibliography{bib}

\end{document}
